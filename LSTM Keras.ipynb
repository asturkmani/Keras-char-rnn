{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import lxml.etree\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout, GRU\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.wrappers import TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.flags\n",
    "FLAGS.data_path = \"talks.txt\"\n",
    "FLAGS.maxlen = 50\n",
    "FLAGS.batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Download Dataset\n",
    "if not os.path.isfile('ted_en-20160408.zip'):\n",
    "    urllib.request.urlretrieve(\"https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\", filename=\"ted_en-20160408.zip\")\n",
    "\n",
    "\n",
    "# Extract documents   \n",
    "with zipfile.ZipFile('ted_en-20160408.zip', 'r') as z:\n",
    "    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preprocess data and remove characters we don't want in our vocabulary. This will speed up learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "corpus = \"\"\n",
    "totalcorpus = \"\"\n",
    "i=0\n",
    "chars_to_remove = ['+', ',', '-','/','<', '=', '>','@', '[', '\\\\', ']', '^', '_','\\x80', '\\x93', '\\x94', '\\xa0', '¡', '¢', '£', '²', 'º', '¿', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'ï', 'ñ', 'ó', 'ô', 'ö', 'ø', 'ù', 'û', 'ü', 'ā', 'ă', 'ć', 'č', 'ē', 'ě', 'ī', 'ō', 'ť', 'ū', '˚', 'τ', 'ย', 'ร', 'อ', '่', '€', '∇', '♪', '♫', '你', '葱', '送', '–', '—', '‘', '’', '“', '”','0', '1', '2', '3', '4', '5', '6', '7', '8', '9','#', '$', '%', '&', '!', '\"', \"'\", '(', ')', '*', ':', ';','…']\n",
    "rx = '[' + re.escape(''.join(chars_to_remove)) + ']'\n",
    "for document in doc.findall('//content'):\n",
    "    i +=1\n",
    "    # get each talk\n",
    "    corpus = document.text.lower()\n",
    "    # remove unwanted characters\n",
    "    corpus = re.sub(rx, '', corpus)\n",
    "    # create total corpus\n",
    "    totalcorpus = totalcorpus + \" S \" + corpus + \" E \"\n",
    "print(len(totalcorpus))\n",
    "print(i)\n",
    "corpus = totalcorpus\n",
    "\n",
    "with open(FLAGS.data_path, \"w\") as text_file:\n",
    "    text_file.write(corpus) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load text file if processed before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def readfile(data_path):\n",
    "    corpus = open(data_path, \"r\")\n",
    "    corpus = corpus.read()\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create character to index and index to character functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create dictionaries to convert characters to indices and vice-versa\n",
    "def get_dicts(corpus):\n",
    "    chars = sorted(list(set(corpus)))\n",
    "    char2ind = dict((c, i) for i, c in enumerate(chars))\n",
    "    ind2char = dict((i, c) for i, c in enumerate(chars))\n",
    "    return char2ind, ind2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split text into overlapping sentences with step size 3.\n",
    "print('Splitting text into sequences...')\n",
    "def split2sentences(corpus, maxlen):\n",
    "    sentencelen = maxlen+1\n",
    "    step = 5\n",
    "    sentences = []\n",
    "    for i in range(0, len(corpus) - sentencelen, step):\n",
    "        sentences.append(corpus[i: i + sentencelen])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create function to convert indices to one-hot encoding\n",
    "def vectorize(sentences, maxlen, charlen, char_indices):\n",
    "    X = np.zeros((len(sentences), maxlen, charlen), dtype=np.bool)\n",
    "    Y = np.zeros((len(sentences), maxlen, charlen), dtype=np.bool)\n",
    "    \n",
    "    # vectorize the entire set by splitting sentences into X and Y, where Y is X shifted\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            if t==0:\n",
    "                X[i, t, char_indices[char]] = 1\n",
    "            elif t==50:\n",
    "                Y[i, t-1, char_indices[char]] = 1\n",
    "            else:\n",
    "                X[i, t, char_indices[char]] = 1\n",
    "                Y[i, t-1, char_indices[char]] = 1\n",
    "    return X,Y\n",
    "\n",
    "def unvectorize(tensor, ind2char):\n",
    "    sentences = []\n",
    "    print(tensor.shape)\n",
    "    for i, sentence in enumerate(tensor):\n",
    "        x = \"\"\n",
    "        for j, char in enumerate(sentence):\n",
    "            y = ind2char[np.argmax(char)]\n",
    "            x += y\n",
    "        sentences.append(x)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data generator to pass batches to model\n",
    "def generate_data(FLAGS):\n",
    "    corpus = readfile(FLAGS.data_path)\n",
    "    char2ind, ind2char = get_dicts(corpus)\n",
    "    sentences = split2sentences(corpus, FLAGS.maxlen)\n",
    "    num_sent = len(sentences)\n",
    "    while 1:\n",
    "        for j in range(0, num_sent, batch_size):\n",
    "            batch = sentences[j:j+batch_size]\n",
    "            X, Y = vectorize(batch, FLAGS.maxlen, len(char2ind), char2ind)\n",
    "            yield (X, Y)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "FLAGS.samples_per_epoch = len(split2sentences(readfile(FLAGS.data_path), FLAGS.maxlen))\n",
    "char2ind, ind2char = get_dicts(corpus=readfile(FLAGS.data_path))\n",
    "FLAGS.charlen = len(char2ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# RNN parameters\n",
    "N_HIDDEN = 512\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Building training model...')\n",
    "model = Sequential()\n",
    "model.add(GRU(N_HIDDEN, input_shape=(maxlen, len(char2ind))))\n",
    "model.add(TimeDistributed(Dense(N_HIDDEN2)))\n",
    "model.add(TimeDistributed(Activation('relu'))) \n",
    "model.add(TimeDistributed(Dense(FLAGS.charlen)))  # Add another dense layer with the desired output size.\n",
    "model.add(TimeDistributed(Activation('softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer = RMSprop(lr=0.002, clipnorm=5))\n",
    "\n",
    "print(model.summary()) # Convenient function to see details about the network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build inference model\n",
    "Note: the inference model will have only one time step as we will feed each predicted character back into the rnn as a seed for predicting the next character. It will also be stateful so as to 'remember' previous states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Building Inference model...')\n",
    "inference_model = Sequential()\n",
    "\n",
    "inference_model.add(LSTM(N_HIDDEN, batch_input_shape=(1, 1, len(char2id)), stateful = True))\n",
    "# Since the above LSTM does not output sequences, we don't need TimeDistributed anymore.\n",
    "inference_model.add(Dense(N_HIDDEN2))\n",
    "inference_model.add(Activation('relu'))\n",
    "inference_model.add(Dense(FLAGS.charlen))\n",
    "inference_model.add(Activation('softmax'))\n",
    "\n",
    "# Copy the weights of the trained network. Both should have the same exact number of parameters (why?).\n",
    "inference_model.load_weights('model.h5')\n",
    "\n",
    "# Given the start Character 'S' (one-hot encoded), predict the next most likely character.\n",
    "startChar = np.zeros((1, 1, FLAGS.charlen))\n",
    "startChar[0, 0, char2id['S']] = 1\n",
    "nextCharProbabilities = inference_model.predict(startChar)\n",
    "\n",
    "# print the most probable character that goes next.\n",
    "print(id2char[nextCharProbabilities.argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Run training and save generated text (1000 characters) to output file. \n",
    "Save model after every iteration to be able to stop and restart training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epoch=0\n",
    "print('Training model')\n",
    "while 1:\n",
    "    epoch += 1\n",
    "    t0 = time.time()\n",
    "    model.reset_states()\n",
    "    hist = model.fit_generator(generate_data(FLAGS), nb_epoch=1, samples_per_epoch=FLAGS.samples_per_epoch)\n",
    "    model.save_weights(\"model.h5\")\n",
    "    t1 = time.time()\n",
    "    total = t1-t0\n",
    "    \n",
    "    \n",
    "    if epoch%5 == 0:\n",
    "        orig_stdout = sys.stdout\n",
    "        f = open('output.txt', 'a+')\n",
    "        sys.stdout = f\n",
    "\n",
    "        print(\"------------- EPOCH\" + str(epoch) + \" ----------------\")\n",
    "        print('Time taken: ')\n",
    "        print(total)\n",
    "\n",
    "        # Copy the weights of the trained network. Both should have the same exact number of parameters (why?).\n",
    "        inference_model.load_weights('model.h5')\n",
    "        inference_model.reset_states()\n",
    "        # Given the start Character 'S' (one-hot encoded), predict the next most likely character.\n",
    "        startChar = np.zeros((1, 1, FLAGS.charlen))\n",
    "        startChar[0, 0, char2ind['S']] = 1\n",
    "        text=\"\"\n",
    "        for i in range(1000):\n",
    "            nextCharProbs = inference_model.predict(startChar)\n",
    "            nextCharProbs = np.asarray(nextCharProbs).astype('float64') # Weird type cast issues if not doing this.\n",
    "            nextCharProbs = nextCharProbs / nextCharProbs.sum()  # Re-normalize for float64 to make exactly 1.0.\n",
    "\n",
    "            nextCharId = np.random.multinomial(1, nextCharProbs.squeeze(), 1).argmax()\n",
    "            text += ind2char[nextCharId] # The comma at the end avoids printing a return line character.\n",
    "            startChar.fill(0)\n",
    "            startChar[0, 0, nextCharId] = 1\n",
    "        print('Generated Text:')\n",
    "        print(text)\n",
    "        sys.stdout = orig_stdout\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
